# PDF批量分析报告

生成时间: 2025-11-24 11:30:11

总计分析: 30 个PDF文件

---

## 1. Chi et al. - 2024 - Universal Manipulation Interface In-The-Wild Robot Teaching Without In-The-Wild Robots.pdf

**分析结果**:

1. 该研究开发了一套基于UMI（Universal Manipulation Interface）的机器人教学框架，能够直接从野外人类演示中学习有效的机器人政策，适用于多种复杂的动态、双臂、精确和长时任务。

2. 该研究使用了成功率作为主要指标，比较了UMI框架在数据采集质量、数据采集效率、通用化能力等方面的能力。具体来说，通过不同任务的成功率评估了UMI框架在数据采集质量上的表现；通过与人类手部演示和典型遥操作界面的比较，评估了数据采集效率；通过在不同环境和对象上的测试，评估了通用化能力。

3. 研究在多种真实世界任务上进行了实验，包括单手、双臂、动态和长时任务。具体实验包括：放置咖啡杯、动态投掷、双臂布折叠和洗碗等任务。实验在不同环境中进行，包括咖啡桌、水龙头等，以评估UMI框架的通用化能力。

4. 研究得出的结论是，UMI框架能够显著提高数据采集效率，同时保持高度的通用性，适用于多种机器人平台和任务。UMI框架能够在不同环境中和对象上实现零样本泛化，成功率达到70%，远超其他行为克隆框架。

*Token使用: 18786*

---

## 2. Ha et al. - 2024 - UMI on Legs Making Manipulation Policies Mobile with Manipulation-Centric Whole-body Controllers.pdf

**分析结果**:

1. 该研究提出了一种结合现实世界和模拟数据的新框架UMI-on-Legs，用于四足机器人的抓取操作。具体而言，该框架利用手持式夹具（UMI）在现实世界中进行任务导向的数据收集，并通过大规模并行模拟训练全身控制器，从而实现跨体态的移动抓取操作。

2. 研究使用了成功率作为主要的评估指标，比较了系统在复杂抓取、非抓取全身体操和动态操作等任务上的表现。通过与基座固定机器人手臂的预训练策略进行零样本跨体态部署，验证了该框架的有效性。

3. 研究设计了一系列实验来验证关键设计决策，包括验证系统能力（能否学习复杂的抓取技能）、鲁棒性（能否处理意外的动力学扰动和物体动力学）以及可扩展性（能否使用完全无机器人收集的数据进行跨体态操作）。实验包括动态投掷、全身体操推举和现实世界中的杯盘重新排列等任务。

4. 研究结论是，UMI-on-Legs框架提供了一种可扩展的路径，用于在动态机器人体态上学习表达性的抓取技能。通过结合现实世界的人类演示和模拟强化学习，该框架允许直观的演示，同时能够表示复杂的抓取技能，如动态投掷和推举，以及精确的拾取和放置操作。

*Token使用: 14135*

---

## 3. Liu et al. - 2025 - FastUMI-100K Advancing Data-driven Robotic Manipulation with a Large-scale UMI-style Dataset.pdf

**错误**: 'utf-8' codec can't encode characters in position 11439-11440: surrogates not allowed

---

## 4. Rayyan et al. - 2025 - MV-UMI A Scalable Multi-View Interface for Cross-Embodiment Learning.pdf

**分析结果**:

1. 该研究开发了一套基于手持抓取器的多视角数据采集框架MV-UMI，该框架结合了第一人称视角和第三人称视角，以提高跨体态学习任务中的政策性能。

2. 该研究使用了任务成功率作为评估指标，比较了单视角UMI系统和多视角MV-UMI系统在不同任务中的表现，包括瓶子架插入任务、标记杯放置任务和易拉罐货架放置任务。

3. 该研究在多种环境中进行了实验，包括在不同场景下收集数据，并在实际任务中测试了MV-UMI系统的性能。实验结果显示，多视角系统在处理复杂任务时表现更好，尤其是在目标离开第一人称视角范围后仍能保持较高的成功率。

4. 研究结论是，MV-UMI框架通过结合第一人称视角和第三人称视角，提高了跨体态学习任务中的政策性能，特别是在处理需要更广泛场景理解的子任务时，性能提高了约47%。此外，该框架还减少了由于视角限制导致的分布偏移，从而增强了跨体态兼容性。

*Token使用: 9109*

---

## 5. Gupta et al. - 2025 - UMI-on-Air Embodiment-Aware Guidance for Embodiment-Agnostic Visuomotor Policies.pdf

**分析结果**:

1. 该研究提出了一种基于UMI的Embodiment-Aware Diffusion Policy（EADP），通过结合高阶和低阶控制器的反馈，使通用的视觉-运动策略能够适应特定的机器人载体，从而实现跨载体的鲁棒部署。

2. 该研究使用成功率作为评价指标，比较了在不同机器人载体上的执行能力，包括UR10e机械臂、固定翼无人机UAM及其在扰动条件下的表现。此外，还通过交叉环境泛化测试评估了UMI-on-Air框架在未见过的环境中的应用能力。

3. 研究设计了模拟和真实世界实验。在模拟实验中，评估了不同任务和载体上的策略执行情况，包括开门取物、孔中插钉、旋转阀门和拾取放置等任务。在真实世界实验中，研究了在无人机上执行精确插钉、柠檬采摘和灯泡安装等任务的能力。

4. 研究表明，EADP能够显著减少不同载体之间的执行差距，特别是在控制能力较弱的UAM上，成功率提高了9%以上。在交叉环境泛化测试中，UMI-on-Air框架也成功地在未见过的环境中执行了任务，验证了其在不同环境中的鲁棒性和泛化能力。

*Token使用: 11767*

---

## 6. Xu et al. - 2025 - exUMI Extensible Robot Teaching System with Action-aware Task-agnostic Tactile Representation.pdf

**分析结果**:

1. 该研究开发了一套基于UMI的增强数据采集方案exUMI，该方案结合了AR动作捕捉系统和磁性旋转编码器，以提高机械臂末端执行器的姿态和宽度的精确跟踪，并通过随机与物体交互收集大量触觉-动作对齐的数据，用于触觉表示学习。

2. 该研究使用了预测均方误差（Prediction MSE Error）作为评估指标，比较了触觉表示学习的质量。具体而言，通过比较不同输入设置下的预测误差，展示了多模态条件下的触觉预测性能提升，特别是在使用行动感知预测预训练（TPP）方法时，触觉表示学习的性能显著提高。

3. 该研究进行了多项实验，包括触觉预测预训练和模仿学习实验。在触觉预测预训练实验中，研究者在收集到的大量数据集上预训练了TPP模型，并通过比较不同设置下的预测误差，验证了TPP方法的有效性。在模仿学习实验中，研究者评估了触觉感知策略在复杂任务中的性能，展示了TPP方法在提高操作成功率方面的显著优势。

4. 研究结论是，exUMI系统通过结合先进的硬件和算法，显著提高了触觉数据采集的可靠性和效率，使得触觉表示学习在复杂任务中的性能大幅提升。TPP方法通过行动感知的触觉预测预训练，有效缓解了触觉数据稀疏性问题，并在多种实际机器人任务中展示了显著的性能改进。

*Token使用: 13959*

---

## 7. Zhaxizhuoma et al. - 2025 - FastUMI A Scalable and Hardware-Independent Universal Manipulation Interface with Dataset.pdf

**分析结果**:

1. 做了什么东西？FastUMI 是一套基于UMI的重新设计系统，用于简化现实世界数据采集，提高灵活性和适应性。它采用解耦硬件设计，引入了标准化的插拔式指尖附件和灵活的相机安装结构，同时使用RealSense T265模块替代复杂的视觉惯性里程计（VIO），以提高数据采集的稳定性和准确性。

2. 用了什么metric，比较了什么能力？通过比较数据采集质量（如定位误差）、基线性能（如ACT和DP算法的成功率）、算法增强效果（如深度增强DP和Smooth-ACT、PoseACT算法）以及额外因素（如不同相机配置和训练数据量对政策推理性能的影响），评估了FastUMI在数据采集质量和算法适应性等方面的能力。

3. 做了什么实验？实验包括评估T265和MINI模块在姿态跟踪中的可靠性、在多种操作任务中展示系统的有效性、量化算法改进带来的成功率提升、以及考察不同相机配置和训练数据规模对政策推理性能的影响。

4. 结论是什么？FastUMI能够降低数据采集成本，简化部署过程，并支持大规模数据驱动的策略学习。通过引入标准化的插拔式指尖附件和灵活的相机安装结构，FastUMI提高了数据采集的灵活性和适应性，同时通过深度增强DP和Smooth-ACT、PoseACT算法等改进，提高了策略执行的鲁棒性和准确性。

*Token使用: 18911*

---

## 8. Heng et al. - 2025 - RwoR Generating Robot Demonstrations from Human Hand Collection for Policy Learning without Robot.pdf

**分析结果**:

1. 该研究开发了一套基于人类手部演示的数据采集方案，利用GoPro鱼眼相机捕捉人类手部演示，并通过训练的手到夹爪生成模型将手部演示转化为UMI夹爪演示，从而有效填补了人类手部与实际机器人观察之间的视觉差距，实现高效的数据采集。

2. 该研究使用了成功率作为评估指标，比较了使用该方案生成的机器人夹爪演示与直接使用手持UMI夹爪采集的数据生成的机器人夹爪演示之间的性能，验证了生成的机器人夹爪演示的质量和有效性。

3. 该研究在实际机器人上进行了部署实验，评估了整个框架的性能，并在不同任务和背景场景下验证了生成的机器人夹爪演示的质量。实验结果显示，该方案能够生成高质量的机器人夹爪演示，并在多种任务中表现出良好的成功率。

4. 该研究的结论是，通过利用人类手部演示生成高质量的UMI夹爪演示，该方案能够有效提高数据采集的质量和效率，同时实现操作空间的灵活性和可扩展性，适用于多种机器人操作任务。

*Token使用: 13685*

---

## 9. Zeng et al. - 2025 - ActiveUMI Robotic Manipulation with Active Perception from Robot-Free Human Demonstrations.pdf

**错误**: Error code: 429 - {'message': 'Request was rejected due to rate limiting. Details: TPM limit reached.', 'data': None}

---

## 10. Fu et al. - 2024 - Mobile ALOHA Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation.pdf

**分析结果**:

1. 该研究开发了一套名为Mobile ALOHA的低成本双臂移动操作系统，该系统结合了双臂远程操作和移动底座，能够进行复杂的家务操作和移动任务，如擦桌子、烹饪虾、使用橱柜等。

2. 该研究使用了成功率作为主要的评估指标，比较了使用Mobile ALOHA进行数据采集与仅使用Mobile ALOHA数据进行训练的能力，以及与使用静态ALOHA数据进行训练的能力。此外，还评估了数据效率和不同模仿学习方法的性能。

3. 该研究进行了多项实验，包括在真实环境中执行复杂的移动操作任务，如擦桌子、烹饪虾、使用橱柜等。实验还测试了Mobile ALOHA在不同任务上的表现，如打开双门橱柜、呼叫电梯、推椅子等。

4. 研究结论表明，使用静态ALOHA数据进行协同训练可以显著提高Mobile ALOHA在移动操作任务上的成功率，尤其是在需要精确操作的任务中，如按按钮、翻转虾等。此外，与仅使用Mobile ALOHA数据进行训练相比，协同训练提高了数据效率，并且Mobile ALOHA能够执行多种复杂的移动操作任务。

*Token使用: 19243*

---

## 11. Team et al. - 2024 - Octo An Open-Source Generalist Robot Policy.pdf

**分析结果**:

1. 该研究开发了一种通用机器人政策Octo，该政策基于Transformer架构，并在包含80万个机器人演示的大型数据集上进行了预训练。Octo支持灵活的任务和观察定义，并能够快速适应新的观察和动作空间。

2. 该研究使用成功率作为评估指标，比较了Octo在多种机器人设置下的零样本控制能力以及数据高效微调能力。此外，还评估了Octo在不同任务和新观察输入、新动作空间上的适应能力。

3. 该研究在9个不同的机器人平台上进行了广泛的实验，包括单臂和双臂操作，评估了Octo的初始化性能以及在新设置下的微调效果。实验覆盖了多种任务，如Berkeley Peg Insertion、Stanford Coffee等。

4. 研究结论是Octo作为一种通用机器人政策，能够有效初始化并快速适应新的观察和动作空间，展示了在多种机器人设置下出色的表现。此外，Octo还提供了全面的训练和微调代码，便于研究人员和实践者使用大型机器人数据集进行高效学习和广泛泛化。

*Token使用: 22135*

---

## 12. Khazatsky et al. - 2025 - DROID A Large-Scale In-The-Wild Robot Manipulation Dataset.pdf

**分析结果**:

1. 该研究制作了一个名为DROID的大型“在野外”机器人操作数据集，包含76000个演示轨迹或350小时的交互数据，覆盖564个场景、52栋建筑和86项任务。数据集通过18个研究实验室在北美、亚洲和欧洲12个月内收集，使用相同的机器人硬件堆栈（基于流行的Franka Panda机器人臂）进行数据采集。

2. 该研究使用了成功轨迹数和平均性能提升作为评估指标，比较了使用DROID数据集训练的策略与使用现有大规模机器人操作数据集训练的策略在任务成功率和鲁棒性方面的表现。结果显示，使用DROID数据集训练的策略在平均性能上提高了约20%。

3. 该研究在6个任务和4个不同地点（实验室、办公室和真实家庭）进行了实验，包括从简单的抓取放置任务到多阶段烹饪任务，以评估DROID数据集在各种机器人操作任务和环境中的性能和鲁棒性。

4. 研究结论是DROID数据集能够显著提高策略的成功率和鲁棒性，特别是在分布外（OOD）设置下表现更为突出。此外，DROID数据集的多样性和广泛性有助于提升策略的泛化能力，使其在新的场景和任务中表现更好。

*Token使用: 19016*

---

## 13. Cheng et al. - 2024 - Open-TeleVision Teleoperation with Immersive Active Visual Feedback.pdf

**分析结果**:

1. 该研究开发了一套基于VR设备的沉浸式遥操作系统Open-TeleVision，允许操作者通过头戴式主动摄像机和上身动作控制机器人，实现精细的手部和手臂操作。

2. 该研究使用成功率和完成时间作为评价指标，比较了不同系统在四类精细任务（罐子分类、罐子插入、折叠毛巾、卸载）上的表现，包括单目与立体视觉输入、不同硬件平台以及远程操作能力。

3. 该研究通过用户实验验证了系统的有效性，参与者完成了四个任务，包括罐子分类、罐子插入、折叠毛巾和卸载操作，结果显示立体视觉输入显著优于单目输入，且操作时间更短，成功率更高。此外，研究还展示了远程操作能力，即操作者可以通过互联网远程控制位于另一地点的机器人。

4. 研究结论是，Open-TeleVision系统能够显著提高数据采集的质量和效率，特别是对于精细操作任务，同时提供更直观的空间感知和远程操作能力，使遥操作更加灵活和高效。

*Token使用: 14493*

---

## 14. Fu et al. - 2024 - HumanPlus Humanoid Shadowing and Imitation from Humans.pdf

**分析结果**:

1. 该研究开发了一套基于全栈系统的类人机器人学习系统，使机器人能够从人类数据中学习运动和自主技能。具体来说，该系统包括从人类操作者实时模仿人类运动的低级策略，以及通过双目视觉技能策略进行模仿学习，使机器人能够自主完成多种任务。

2. 该研究使用成功率作为主要评估指标，比较了不同方法在多种任务中的表现，包括穿戴鞋子并行走、整理物体、折叠衣物、打字和与另一机器人打招呼等任务。此外，还通过用户研究比较了不同远程操作系统的效率和稳定性。

3. 该研究通过一系列实验验证了系统的有效性和实用性。实验包括穿戴鞋子并行走、整理物体、折叠衣物、打字和与另一机器人打招呼等任务，展示了系统在多种复杂任务中的应用能力。还进行了用户研究，比较了不同远程操作系统的效率和稳定性。

4. 该研究的结论是，所提出的系统能够使类人机器人从人类数据中高效学习复杂的自主技能，特别是通过实时模仿和双目视觉技能策略，机器人能够完成多种任务，如穿戴鞋子并行走、整理物体、折叠衣物等，成功率高达60%-100%。此外，该系统还展示了在用户操作效率和稳定性方面的优势。

*Token使用: 17470*

---

## 15. He et al. - 2024 - OmniH2O Universal and Dexterous Human-to-Humanoid Whole-Body Teleoperation and Learning.pdf

**分析结果**:

1. 该论文提出了一套基于学习的方法（OmniH2O）来实现全尺寸人形机器人的远程操作和自主控制，包括通过VR头盔、口头指令和RGB摄像头等多种方式进行远程操作，并通过生成模型或模仿学习方法实现自主控制。

2. 该研究使用了多种评估指标，包括成功率（Succ）、全局MPJPE（Eg-mpjpe）和局部MPJPE（Empjpe）、关节加速度误差（Eacc）和速度误差（Evel）。实验比较了不同方法在模拟和真实环境中的运动跟踪能力，以及在使用不同历史步数和神经网络架构时的表现。

3. 该论文进行了多项实验，包括在模拟环境中和真实环境中对全尺寸人形机器人进行运动跟踪，评估了不同方法的性能。还展示了通过远程操作和自主控制实现多种日常任务的能力，如挥动球拍、浇水、刷字、蹲下拾物、拳击、篮子投递等。此外，还进行了自主控制实验，通过与GPT-4o集成来实现基于视觉输入的自主控制。

4. 该研究的主要结论是，OmniH2O系统能够实现精确的全尺寸人形机器人远程操作和自主控制，支持多种控制接口，并能够通过模仿学习方法从远程操作数据中学习自主策略。此外，该系统在模拟和真实环境中的运动跟踪性能显著优于现有方法，展示了其在复杂任务中的适用性和鲁棒性。

*Token使用: 15993*

---

## 16. Yang et al. - 2024 - ACE A Cross-Platform Visual-Exoskeletons System for Low-Cost Dexterous Teleoperation.pdf

**错误**: Error code: 429 - {'message': 'Request was rejected due to rate limiting. Details: TPM limit reached.', 'data': None}

---

## 17. Fang et al. - 2024 - AirExo Low-Cost Exoskeletons for Learning Whole-Arm Manipulation in the Wild.pdf

**分析结果**:

1. 该研究开发了一种低成本、可穿戴、适应性强的双臂外骨骼系统AirExo，用于远程操作和演示数据收集，以实现仿人双臂操作学习。AirExo能够模拟双臂机器人的运动学，支持在自然环境中收集大规模的在野演示数据，从而辅助机器人学习复杂的双臂操作任务。

2. 该研究使用了任务完成率、成功率和碰撞率等指标来评估不同方法在双臂操作任务中的性能。通过比较使用不同数量的在野和远程操作演示数据训练的策略，研究展示了使用在野演示数据可以显著提高策略的学习效率和泛化能力。

3. 研究通过两个双臂操作任务（收集球和从窗帘后面抓取物品）进行了实验。在收集了50个和10个远程操作演示数据后，研究展示了使用AirExo收集的在野演示数据可以显著提高策略的学习效率，并在任务后期阶段实现更高的成功率。此外，研究还设计了扰动实验来评估策略的鲁棒性。

4. 研究结论是AirExo能够显著减少对昂贵的远程操作演示数据的需求，通过结合在野演示数据，策略可以实现更高的泛化能力和鲁棒性，特别是在多阶段双臂操作任务中。这表明使用AirExo收集的在野演示数据可以有效提高机器人学习复杂操作任务的能力。

*Token使用: 13989*

---

## 18. Fang et al. - 2025 - AirExo-2 Scaling up Generalizable Robotic Imitation Learning with Low-Cost Exoskeletons.pdf

**分析结果**:

1. 该研究开发了一种基于低成本外骨骼系统的AirExo-2，用于大规模采集和适应野外演示数据，以实现通用的机器人模仿学习。
2. 该研究使用了数据采集质量和数据采集效率作为评估指标，比较了AirExo-2与传统远程操作采集方法在数据采集质量、视觉一致性、动作准确性以及采集成本等方面的性能。
3. 该研究进行了用户研究和系统分析实验，分别评估了AirExo-2在用户学习操作难度、操作便捷性、动作准确性以及采集效率等方面的表现。此外，还进行了复杂任务实验，验证了AirExo-2在执行复杂任务时的有效性。
4. 研究结论是AirExo-2能够显著提高数据采集效率，降低数据采集成本，同时保持高质量的数据有效性，适用于大规模野外数据采集，并且能够有效执行复杂的机器人操作任务。

*Token使用: 18294*

---

## 19. Etukuru et al. - 2024 - Robot Utility Models General Policies for Zero-Shot Deployment in New Environments.pdf

**错误**: Error code: 429 - {'message': 'Request was rejected due to rate limiting. Details: TPM limit reached.', 'data': None}

---

## 20. Seo et al. - 2025 - LEGATO Cross-Embodiment Imitation Using a Grasping Tool.pdf

**错误**: Error code: 429 - {'message': 'Request was rejected due to rate limiting. Details: TPM limit reached.', 'data': None}

---

## 21. Duan et al. - 2023 - AR2-D2Training a Robot Without a Robot.pdf

**错误**: Error code: 429 - {'message': 'Request was rejected due to rate limiting. Details: TPM limit reached.', 'data': None}

---

## 22. Chen et al. - 2025 - ARCap Collecting High-Quality Human Demonstrations for Robot Learning with Augmented Reality Feedba.pdf

**错误**: Error code: 429 - {'message': 'Request was rejected due to rate limiting. Details: TPM limit reached.', 'data': None}

---

## 23. Kareer et al. - 2025 - EgoMimic Scaling Imitation Learning via Egocentric Video.pdf

**错误**: Error code: 429 - {'message': 'Request was rejected due to rate limiting. Details: TPM limit reached.', 'data': None}

---

## 24. Kim et al. - 2023 - Training Robots Without Robots Deep Imitation Learning for Master-to-Robot Policy Transfer.pdf

**分析结果**:

1. 该研究提出了一种无需使用机器人进行教学的新型主-从政策转移系统，通过人类直接使用控制器进行任务演示，控制器模仿了机器人手臂的运动学参数，并配备了力/扭矩传感器以测量力反馈。该系统结合了基于凝视的视觉注意力、简单的校准方法和基于Transformer的力/扭矩传感器注意力机制，以实现从人类到机器人的政策转移。

2. 该研究使用成功率作为评价指标，比较了不同神经网络架构在执行瓶盖开启任务时的表现。通过实验验证了基于凝视的双动作方法、力/扭矩传感器输入、以及基于Transformer的力/扭矩传感器注意力机制的有效性。

3. 该研究在真实机器人上进行了瓶盖开启任务的实验，通过重复18次不同初始位置的测试，评估了机器人能否准确抓取瓶子、抓取瓶盖并旋转瓶盖。实验中还对比了不同神经网络架构的表现，包括DA-force、No-force、No-DA和No-gaze四种架构。

4. 研究结论表明，提出的基于凝视的双动作方法结合力/扭矩传感器输入和基于Transformer的力/扭矩传感器注意力机制，在瓶盖开启任务中表现出最佳的累积成功率（83.3%）。此外，该方法能够有效抑制视觉和运动学差距，无需使用机器人进行演示，且成本较低，操作简单安全，具有良好的泛化能力。

*Token使用: 10057*

---

## 25. Lepert et al. - 2025 - Masquerade Learning from In-the-wild Human Videos using Data-Editing.pdf

**错误**: Error code: 429 - {'message': 'Request was rejected due to rate limiting. Details: TPM limit reached.', 'data': None}

---

## 26. Feng et al. - 2025 - Learning Dexterous Manipulation with Quantized Hand State.pdf

**错误**: Error code: 429 - {'message': 'Request was rejected due to rate limiting. Details: TPM limit reached.', 'data': None}

---

## 27. Yu et al. - 2025 - EgoMI Learning Active Vision and Whole-Body Manipulation from Egocentric Human Demonstrations.pdf

**分析结果**:

1. 该研究开发了一套名为EgoMI的框架，用于捕捉同步的手部和头部运动轨迹，从而能够将人类演示中的手眼协调行为转移到兼容的半人形机器人上。

2. 研究使用了成功率作为评价指标，比较了具有显式头部运动建模的策略与基线方法在双臂机器人上的表现。通过桌面任务和货架任务的实验，验证了EgoMI框架在宽范围双臂操作中的优势。

3. 实验主要在两个任务中进行：桌面搜索任务和货架搜索任务。在桌面搜索任务中，机器人需要在宽广的工作空间中找到目标物体并放置；在货架搜索任务中，机器人需要在高架储物柜中进行垂直和水平搜索，并执行精确的双臂交接动作。

4. 研究结论是EgoMI框架能够显著提高策略在真实机器人上的迁移性能，无需额外的视觉增强、显式的视觉对齐或在体数据收集。通过协调手眼学习，EgoMI有效地缩小了人类与机器人之间的体态差距，为半人形机器人上的稳健模仿学习提供了有力支持。

*Token使用: 11975*

---

## 28. Wang et al. - 2025 - FieldGen From Teleoperated Pre-Manipulation Trajectories to Field-Guided Data Generation.pdf

**错误**: Error code: 429 - {'message': 'Request was rejected due to rate limiting. Details: TPM limit reached.', 'data': None}

---

## 29. Zhong et al. - 2025 - HumanoidExo Scalable Whole-Body Humanoid Manipulation via Wearable Exoskeleton.pdf

**错误**: Error code: 429 - {'message': 'Request was rejected due to rate limiting. Details: TPM limit reached.', 'data': None}

---

## 30. Yu et al. - 2025 - ARMADA Autonomous Online Failure Detection and Human Shared Control Empower Scalable Real-world Dep.pdf

**分析结果**:

1. 该研究开发了一种名为ARMADA的多机器人系统，结合了自主在线故障检测方法FLOAT和多机器人共享控制机制，以实现高效的人机协作和适应性控制。

2. 该研究使用了准确率、加权准确率和样本级负预测率等指标来评估故障检测性能，并通过与之前的人机协作学习方法Sirius进行对比，展示了在成功率和人类干预率方面的显著提升。

3. 研究在四个真实世界任务上进行了实验，包括将珠子倒入碗中、挂杯子、抓芒果放入抽屉和折叠毛巾，验证了ARMADA在故障检测和多机器人部署与适应方面的有效性。

4. 研究结论是ARMADA能够显著减少对人类监督的依赖，提高成功率超过四倍，并将人类干预率降低超过两倍，展示了在真实世界部署和适应中的潜在应用价值。

*Token使用: 13551*

---


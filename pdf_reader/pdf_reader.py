#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PDFæ‰¹é‡åˆ†æå·¥å…·
åŠŸèƒ½ï¼š
1. æ‰«ææŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰PDFæ–‡ä»¶
2. ä½¿ç”¨OpenAI APIå¯¹æ¯ä¸ªPDFè¿›è¡Œåˆ†æ
3. æ ¹æ®è‡ªå®šä¹‰promptæå–å…³é”®ä¿¡æ¯
4. ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
"""

import os
import json
from pathlib import Path
from typing import List, Dict, Optional
import logging
from datetime import datetime
import PyPDF2
from openai import OpenAI

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class PDFAnalyzer:
    """PDFæ‰¹é‡åˆ†æå™¨"""
    
    def __init__(self, api_key: Optional[str] = None, base_url: Optional[str] = None):
        """
        åˆå§‹åŒ–PDFåˆ†æå™¨
        
        Args:
            api_key: SiliconFlow APIå¯†é’¥ï¼Œå¦‚æœä¸ºNoneåˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
            base_url: APIåŸºç¡€URLï¼Œé»˜è®¤ä¸ºSiliconFlow
        """
        self.api_key = api_key or os.getenv("SILICONFLOW_API_KEY")
        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ°SiliconFlow APIå¯†é’¥ï¼Œè¯·è®¾ç½®SILICONFLOW_API_KEYç¯å¢ƒå˜é‡æˆ–ä¼ å…¥api_keyå‚æ•°")
        
        self.base_url = base_url or "https://api.siliconflow.cn/v1"
        self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)
        self.default_model = "Qwen/Qwen2.5-7B-Instruct"
    
    def find_all_pdfs(self, folder_path: str) -> List[Path]:
        """
        æŸ¥æ‰¾æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰PDFæ–‡ä»¶
        
        Args:
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„
            
        Returns:
            PDFæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        folder = Path(folder_path)
        if not folder.exists():
            raise ValueError(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
        
        pdf_files = list(folder.glob("**/*.pdf"))
        logger.info(f"åœ¨ {folder_path} ä¸­æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
        
        return sorted(pdf_files)
    
    def extract_text_from_pdf(self, pdf_path: Path, max_pages: int = 10) -> str:
        """
        ä»PDFä¸­æå–æ–‡æœ¬å†…å®¹
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            max_pages: æœ€å¤šæå–çš„é¡µæ•°ï¼ˆé¿å…tokenè¿‡å¤šï¼‰
            
        Returns:
            æå–çš„æ–‡æœ¬å†…å®¹
        """
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                total_pages = len(pdf_reader.pages)
                pages_to_read = min(total_pages, max_pages)
                
                text_content = []
                for page_num in range(pages_to_read):
                    page = pdf_reader.pages[page_num]
                    text = page.extract_text()
                    if text.strip():
                        text_content.append(text)
                
                full_text = "\n\n".join(text_content)
                
                if total_pages > max_pages:
                    logger.warning(f"{pdf_path.name}: åªæå–äº†å‰{max_pages}é¡µï¼ˆå…±{total_pages}é¡µï¼‰")
                
                return full_text
                
        except Exception as e:
            logger.error(f"æå–PDFæ–‡æœ¬å¤±è´¥ {pdf_path.name}: {e}")
            return ""
    
    def analyze_pdf_with_prompt(
        self, 
        pdf_text: str, 
        prompt: str,
        pdf_name: str,
        model: Optional[str] = None
    ) -> Dict[str, str]:
        """
        ä½¿ç”¨OpenAI APIåˆ†æPDFå†…å®¹
        
        Args:
            pdf_text: PDFæ–‡æœ¬å†…å®¹
            prompt: åˆ†ææç¤ºè¯
            pdf_name: PDFæ–‡ä»¶å
            model: ä½¿ç”¨çš„æ¨¡å‹ï¼Œé»˜è®¤ä¸ºgpt-4o-mini
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        if not pdf_text.strip():
            return {
                "status": "error",
                "error": "PDFæ–‡æœ¬ä¸ºç©º",
                "response": ""
            }
        
        try:
            # SiliconFlow API ä½¿ç”¨ OpenAI å…¼å®¹æ ¼å¼
            messages = [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿æå–è®ºæ–‡çš„å…³é”®ä¿¡æ¯ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"
                },
                {
                    "role": "user",
                    "content": f"ä»¥ä¸‹æ˜¯PDFæ–‡ä»¶ã€Š{pdf_name}ã€‹çš„å†…å®¹ï¼š\n\n{pdf_text}\n\n{prompt}"
                }
            ]
            
            logger.info(f"æ­£åœ¨åˆ†æ {pdf_name}...")
            
            response = self.client.chat.completions.create(
                model=model or self.default_model,
                messages=messages,
                temperature=0.3,
                max_tokens=2000
            )
            
            result = response.choices[0].message.content
            
            return {
                "status": "success",
                "response": result,
                "tokens_used": response.usage.total_tokens
            }
            
        except Exception as e:
            logger.error(f"SiliconFlow APIè°ƒç”¨å¤±è´¥ {pdf_name}: {e}")
            return {
                "status": "error",
                "error": str(e),
                "response": ""
            }
    
    def batch_analyze_pdfs(
        self,
        folder_path: str,
        prompt: str,
        output_file: Optional[str] = None,
        max_pages: int = 10,
        model: Optional[str] = None
    ) -> Dict[str, Dict]:
        """
        æ‰¹é‡åˆ†ææ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰PDF
        
        Args:
            folder_path: PDFæ–‡ä»¶å¤¹è·¯å¾„
            prompt: åˆ†ææç¤ºè¯
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            max_pages: æ¯ä¸ªPDFæœ€å¤šæå–çš„é¡µæ•°
            model: ä½¿ç”¨çš„SiliconFlowæ¨¡å‹
            
        Returns:
            æ‰€æœ‰PDFçš„åˆ†æç»“æœ
        """
        # æŸ¥æ‰¾æ‰€æœ‰PDFæ–‡ä»¶
        pdf_files = self.find_all_pdfs(folder_path)
        
        if not pdf_files:
            logger.warning(f"åœ¨ {folder_path} ä¸­æ²¡æœ‰æ‰¾åˆ°PDFæ–‡ä»¶")
            return {}
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"pdf_analysis_{timestamp}.json"
        
        # åˆ†ææ¯ä¸ªPDFï¼ˆå¢é‡ä¿å­˜æ¨¡å¼ï¼‰
        results = {}
        total_tokens = 0
        
        for idx, pdf_path in enumerate(pdf_files, 1):
            logger.info(f"å¤„ç† [{idx}/{len(pdf_files)}]: {pdf_path.name}")
            
            # æå–æ–‡æœ¬
            pdf_text = self.extract_text_from_pdf(pdf_path, max_pages=max_pages)
            
            if not pdf_text.strip():
                logger.warning(f"è·³è¿‡ç©ºæ–‡ä»¶: {pdf_path.name}")
                results[pdf_path.name] = {
                    "status": "skipped",
                    "reason": "æ— æ³•æå–æ–‡æœ¬å†…å®¹"
                }
                # ç«‹å³ä¿å­˜
                self.save_results(results, output_file, total_tokens)
                continue
            
            # è°ƒç”¨APIåˆ†æ
            analysis = self.analyze_pdf_with_prompt(
                pdf_text=pdf_text,
                prompt=prompt,
                pdf_name=pdf_path.name,
                model=model
            )
            
            results[pdf_path.name] = {
                "file_path": str(pdf_path),
                "analysis": analysis,
                "text_length": len(pdf_text)
            }
            
            if analysis.get("tokens_used"):
                total_tokens += analysis["tokens_used"]
            
            # æ¯å¤„ç†å®Œä¸€ä¸ªPDFå°±ç«‹å³ä¿å­˜
            self.save_results(results, output_file, total_tokens)
            logger.info(f"âœ… å·²ä¿å­˜è¿›åº¦: {idx}/{len(pdf_files)}")
        
        logger.info(f"ğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        return results
    
    def save_results(self, results: Dict, output_file: str, total_tokens: int):
        """
        ä¿å­˜åˆ†æç»“æœåˆ°JSONæ–‡ä»¶ï¼ˆå¢é‡ä¿å­˜ï¼‰
        
        Args:
            results: åˆ†æç»“æœ
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            total_tokens: æ€»tokenä½¿ç”¨é‡
        """
        output_data = {
            "timestamp": datetime.now().isoformat(),
            "total_files": len(results),
            "total_tokens_used": total_tokens,
            "results": results
        }
        
        # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ï¼Œé¿å…å†™å…¥è¿‡ç¨‹ä¸­æ–­å¯¼è‡´æ–‡ä»¶æŸå
        temp_file = output_file + ".tmp"
        with open(temp_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        # åŸå­æ€§æ›¿æ¢
        import shutil
        shutil.move(temp_file, output_file)
    
    def generate_markdown_report(self, results: Dict, output_file: str):
        """
        ç”ŸæˆMarkdownæ ¼å¼çš„åˆ†ææŠ¥å‘Š
        
        Args:
            results: åˆ†æç»“æœ
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("# PDFæ‰¹é‡åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"æ€»è®¡åˆ†æ: {len(results)} ä¸ªPDFæ–‡ä»¶\n\n")
            f.write("---\n\n")
            
            for idx, (pdf_name, data) in enumerate(results.items(), 1):
                f.write(f"## {idx}. {pdf_name}\n\n")
                
                if data.get("status") == "skipped":
                    f.write(f"**çŠ¶æ€**: è·³è¿‡\n\n")
                    f.write(f"**åŸå› **: {data.get('reason', 'æœªçŸ¥')}\n\n")
                else:
                    analysis = data.get("analysis", {})
                    if analysis.get("status") == "success":
                        f.write(f"**åˆ†æç»“æœ**:\n\n")
                        f.write(f"{analysis.get('response', 'æ— ç»“æœ')}\n\n")
                        f.write(f"*Tokenä½¿ç”¨: {analysis.get('tokens_used', 0)}*\n\n")
                    else:
                        f.write(f"**é”™è¯¯**: {analysis.get('error', 'æœªçŸ¥é”™è¯¯')}\n\n")
                
                f.write("---\n\n")
        
        logger.info(f"MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")


def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    import argparse
    
    parser = argparse.ArgumentParser(description="PDFæ‰¹é‡åˆ†æå·¥å…·")
    parser.add_argument("folder_path", help="PDFæ–‡ä»¶å¤¹è·¯å¾„")
    parser.add_argument("-p", "--prompt", help="åˆ†ææç¤ºè¯", default=None)
    parser.add_argument("-o", "--output", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆJSONï¼‰", default=None)
    parser.add_argument("-m", "--markdown", help="ç”ŸæˆMarkdownæŠ¥å‘Š", action="store_true")
    parser.add_argument("--max-pages", type=int, default=10, help="æ¯ä¸ªPDFæœ€å¤šæå–çš„é¡µæ•°")
    parser.add_argument("--model", default="Qwen/Qwen2.5-7B-Instruct", help="SiliconFlowæ¨¡å‹")
    
    args = parser.parse_args()
    
    # é»˜è®¤æç¤ºè¯
    if args.prompt is None:
        args.prompt = """è¯·åˆ†æè¿™ç¯‡è®ºæ–‡ï¼Œå›ç­”ä»¥ä¸‹é—®é¢˜ï¼š

1. è¿™ç¯‡è®ºæ–‡åšäº†ä»€ä¹ˆï¼Ÿä¸»è¦è´¡çŒ®æ˜¯ä»€ä¹ˆï¼Ÿ
2. ä½¿ç”¨äº†ä»€ä¹ˆè¯„ä¼°æŒ‡æ ‡ï¼ˆmetricsï¼‰ï¼Ÿæ¯”è¾ƒäº†ä»€ä¹ˆèƒ½åŠ›ï¼Ÿ
3. è¿›è¡Œäº†ä»€ä¹ˆå®éªŒï¼Ÿåœ¨ä»€ä¹ˆåœºæ™¯ä¸‹æµ‹è¯•ï¼Ÿ
4. ä¸»è¦ç»“è®ºæ˜¯ä»€ä¹ˆï¼Ÿ

è¯·ç”¨ç®€æ´çš„ä¸­æ–‡å›ç­”ï¼Œæ¯ä¸ªé—®é¢˜ç”¨ä¸€æ®µè¯æ¦‚æ‹¬ã€‚"""
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = PDFAnalyzer()
    
    # æ‰¹é‡åˆ†æ
    results = analyzer.batch_analyze_pdfs(
        folder_path=args.folder_path,
        prompt=args.prompt,
        output_file=args.output,
        max_pages=args.max_pages,
        model=args.model
    )
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š
    if args.markdown:
        md_output = args.output.replace('.json', '.md') if args.output else f"pdf_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        analyzer.generate_markdown_report(results, md_output)
    
    print(f"\nâœ… åˆ†æå®Œæˆï¼å¤„ç†äº† {len(results)} ä¸ªPDFæ–‡ä»¶")


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    example_folder = "/Users/lr-2002/Downloads/Exported Items/files"
    
    if os.path.exists(example_folder):
        analyzer = PDFAnalyzer()
        
        custom_prompt = """è¯·åˆ†æè¿™ç¯‡è®ºæ–‡ï¼Œå›ç­”ä»¥ä¸‹é—®é¢˜ï¼š

1. åšäº†ä»€ä¹ˆä¸œè¥¿ï¼Ÿï¼ˆä¾‹å¦‚ï¼šä¸€å¥—åŸºäºUMIçš„æ•°æ®é‡‡é›†æ–¹æ¡ˆï¼‰
2. ç”¨äº†ä»€ä¹ˆmetricï¼Œæ¯”è¾ƒäº†ä»€ä¹ˆèƒ½åŠ›ï¼Ÿï¼ˆä¾‹å¦‚ï¼šæ•°æ®é‡‡é›†è´¨é‡ã€æ•°æ®é‡‡é›†æ•ˆç‡ç­‰ï¼‰
3. åšäº†ä»€ä¹ˆå®éªŒï¼Ÿï¼ˆä¾‹å¦‚ï¼šåœ¨å†°ç®±ä¸­è¿›è¡Œæ¢ç´¢ï¼‰
4. ç»“è®ºæ˜¯ä»€ä¹ˆï¼Ÿï¼ˆä¾‹å¦‚ï¼šèƒ½å¤Ÿæé«˜æ•°æ®é‡‡é›†æ•ˆç‡ï¼Œæˆ–è€…æ“ä½œç©ºé—´æ›´åŠ çµæ´»ï¼‰

è¯·ç”¨ç®€æ´çš„ä¸­æ–‡å›ç­”ï¼Œæ¯ä¸ªé—®é¢˜ç”¨ä¸€æ®µè¯æ¦‚æ‹¬ã€‚"""
        
        results = analyzer.batch_analyze_pdfs(
            folder_path=example_folder,
            prompt=custom_prompt,
            max_pages=15
        )
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        analyzer.generate_markdown_report(results, f"pdf_analysis_{timestamp}.md")
        
        print(f"\nâœ… åˆ†æå®Œæˆï¼")
    else:
        print(f"ç¤ºä¾‹æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {example_folder}")
        print("è¯·ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šæ–‡ä»¶å¤¹è·¯å¾„")
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("  python pdf_reader.py <æ–‡ä»¶å¤¹è·¯å¾„>")
        print("  python pdf_reader.py <æ–‡ä»¶å¤¹è·¯å¾„> -p 'è‡ªå®šä¹‰æç¤ºè¯' -m")
